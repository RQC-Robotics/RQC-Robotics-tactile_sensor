train_loss,test_loss
0.08966881795149101,0.059659997522830965
0.05481424762230171,0.05283993154764175
0.05084963240905812,0.049905077517032624
0.04775211333444244,0.046567488461732864
0.04491947244656713,0.04425412952899933
0.04281357014649793,0.042215524315834044
0.04112441682501843,0.04093304455280304
0.039720447675177926,0.039388741105794906
0.03875625206451667,0.038588022738695146
0.03798005705601291,0.03836208030581474
0.037453042605989854,0.037410860508680345
0.03698493470486842,0.03704705312848091
0.03661122002883961,0.036659791469573974
0.03624244942476875,0.03624322459101677
0.0359245669684912,0.0359822116792202
0.03569029023772792,0.035633733421564104
0.03536995043880061,0.03549474388360977
0.03514687306002567,0.0353150337934494
0.03490835038454909,0.03514163225889206
0.03475070662012226,0.03484534338116646
0.034523777946045525,0.03461783856153488
0.03438076116536793,0.03438290238380432
0.034184393388660334,0.03432999774813652
0.034005320774097195,0.03433067768812179
0.033891441641669524,0.034340315908193586
0.03367133761707105,0.033841941654682156
0.0335614042572285,0.03371159687638283
0.03345060864169347,0.03384978905320168
0.03338062107170883,0.03343631774187088
0.033207105939325535,0.033387325406074524
0.033079568096681645,0.03367004796862602
0.032963605534873514,0.03413330763578415
0.032857056922818484,0.03308161288499832
0.032736344804105005,0.03290142960846424
0.032631332180217694,0.03280818402767181
0.0325543177990537,0.032772787287831305
0.03243322315576829,0.0326517915725708
0.03232148083417039,0.032577348574995994
0.03224399500771573,0.03252620823681354
0.03214254085562731,0.0324155555665493
0.032048840628642786,0.03235192373394966
0.03196241818368435,0.03222276754677296
0.031892942566620675,0.032274744138121605
0.0318233160204009,0.032076207473874095
0.031728375828579854,0.03199401028454304
0.03167254776154694,0.03206563860177994
0.031589537968761044,0.03189342141151428
0.03156367349781488,0.032103121057152746
0.0314348562218641,0.03199759110808373
0.03136743651016762,0.03174451120197773
